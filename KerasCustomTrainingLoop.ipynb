{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasCustomTrainingLoop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-778IVk58m",
        "colab_type": "code",
        "outputId": "25604f38-e3c3-4117-db81-3deef50bd9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 49kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc1\n",
            "    Uninstalling tensorflow-2.2.0rc1:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1qA79X_eFy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L3PeSwBhIWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    #paddings = tf.constant([[1, 0,],[2, pad,], [3, pad], [4, 0]])\n",
        "    paddings = tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]])\n",
        "    X_pad = tf.pad(X, paddings, 'CONSTANT')\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    \n",
        "    #print('a_slice_prev: {} '.format(a_slice_prev.shape))\n",
        "    #print('W: {} '.format(W.shape))\n",
        "    s = tf.multiply(a_slice_prev, W)   # element wise product in python\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = tf.math.reduce_sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z =  Z + float(b)\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    \n",
        "    # Retrieve dimensions from W's shape \n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" \n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    n_H = int((n_H_prev + 2*pad -f) // stride ) + 1\n",
        "    n_W = int((n_W_prev + 2*pad -f) // stride ) + 1\n",
        "    \n",
        "    print('Input Shape: {} {} {} {} '.format(m, n_H_prev, n_W_prev, n_C_prev, ))\n",
        "    print('Filter Shape: {} {} {} {} '.format(f, f, n_C_prev, n_C))\n",
        "    print('output Shape: {} {} {} {} '.format(n_H, n_W, stride, pad))\n",
        "    # Initialize the output volume Z with zeros. \n",
        "    #Z = np.zeros(( m, n_H, n_W, n_C ))\n",
        "    #Z = tf.zeros(( m, n_H, n_W, n_C ), tf.float32)\n",
        "    Z = tf.Variable(tf.zeros(( m, n_H, n_W, n_C ), tf.float32), validate_shape=False)\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "    print('Input Shape after pad: {} '.format(A_prev_pad.shape))\n",
        "\n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        #print(A_prev_pad.shape)\n",
        "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
        "        print(a_prev_pad.shape)\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume    \n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    #print(m, h, w, c)\n",
        "                    # Find the corners of the current \"slice\" \n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
        "                    a_slice_prev = a_prev_pad[ vert_start:vert_end, horiz_start:horiz_end, : ]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
        "                    #Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    scalar = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    Z[i, h, w, c].assign(scalar)\n",
        "                    \n",
        "\n",
        "                                            \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvdJbw_UeMKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Gunn2D(layers.Layer):\n",
        "\n",
        "  def __init__(self, input_channels, expansion_rate=32):\n",
        "    super(Gunn2D, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    self.expansion_rate = expansion_rate\n",
        "    self.hparameters = {\"pad\" : 1, \"stride\": 1}\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    print(input_shape)\n",
        "    self.w = self.add_weight(shape=(3, 3, self.input_channels, self.input_channels), initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(shape=(1, 1, 1, self.input_channels), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    #inputs = Conv2D(240, (1, 1), strides = (1, 1), padding='valid', name = 'zGunn')(inputs)\n",
        "    inputs = conv_forward(inputs, self.w, self.b, self.hparameters)\n",
        "    #return tf.matmul(inputs, self.w) + self.b\n",
        "    return inputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7drUaH8ehFk",
        "colab_type": "code",
        "outputId": "dea4d230-fb1e-4046-aa21-8aaf1ad897b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "\n",
        "#x = tf.ones((1, 32, 32, 240))\n",
        "#Gunn2D_layer = Gunn2D(240, 32)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "x = tf.ones((1, 3, 3, 240))\n",
        "Gunn2D_layer = Gunn2D(240, 1)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "y = Gunn2D_layer(x)  # The layer's weights are created dynamically the first time the layer is called\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc1\n",
            "(1, 3, 3, 240)\n",
            "Forward prop : (1, 3, 3, 240)\n",
            "Input Shape: 1 3 3 240 \n",
            "Filter Shape: 3 3 240 240 \n",
            "output Shape: 3 3 1 1 \n",
            "Input Shape after pad: (1, 5, 5, 240) \n",
            "(5, 5, 240)\n",
            "(<tf.Variable 'gunn2d_24/Variable:0' shape=(1, 3, 3, 240) dtype=float32, numpy=\n",
            "array([[[[ 2.8525443 ,  0.13847336, -0.19832578, ..., -1.1304388 ,\n",
            "           0.8817091 ,  0.84695095],\n",
            "         [ 3.5197015 , -0.5759762 , -1.2140291 , ..., -1.0848593 ,\n",
            "           0.8131398 ,  0.9221273 ],\n",
            "         [ 3.51825   , -0.4738339 , -1.1893216 , ...,  0.06820191,\n",
            "           0.5992765 ,  1.1522468 ]],\n",
            "\n",
            "        [[ 3.7265332 ,  2.654409  ,  1.1520985 , ..., -2.4597921 ,\n",
            "           0.6457609 ,  1.9887944 ],\n",
            "         [ 3.308843  ,  3.3597815 , -0.45965186, ..., -2.1983204 ,\n",
            "          -0.04825211,  0.8147798 ],\n",
            "         [ 2.4923732 ,  2.4812703 , -0.6275477 , ..., -0.9573564 ,\n",
            "           0.54244316,  0.6966575 ]],\n",
            "\n",
            "        [[ 2.916656  ,  2.2562003 ,  2.183862  , ..., -1.4147526 ,\n",
            "          -0.6382265 ,  1.6277381 ],\n",
            "         [ 1.5067269 ,  3.7038083 ,  1.39383   , ..., -1.0557506 ,\n",
            "          -1.4906642 ,  1.9937693 ],\n",
            "         [ 0.3179045 ,  2.8672488 ,  1.225064  , ..., -0.16604872,\n",
            "          -0.62953556,  1.5094173 ]]]], dtype=float32)>, (<tf.Tensor: shape=(1, 3, 3, 240), dtype=float32, numpy=\n",
            "array([[[[1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.],\n",
            "         [1., 1., 1., ..., 1., 1., 1.]]]], dtype=float32)>, <tf.Variable 'gunn2d_24/Variable:0' shape=(3, 3, 240, 240) dtype=float32, numpy=\n",
            "array([[[[ 0.03792801, -0.03634157, -0.0193938 , ...,  0.04176897,\n",
            "           0.07064727,  0.07976262],\n",
            "         [-0.06364165, -0.08973695, -0.03804892, ...,  0.05364922,\n",
            "           0.01091166, -0.06971351],\n",
            "         [ 0.00775603,  0.03400408,  0.05240391, ...,  0.0408356 ,\n",
            "           0.00650112,  0.03315751],\n",
            "         ...,\n",
            "         [ 0.10355697,  0.06891834,  0.021006  , ..., -0.03883239,\n",
            "           0.06276157,  0.03045965],\n",
            "         [-0.00596341,  0.00409651, -0.05142067, ..., -0.0384597 ,\n",
            "           0.10381969,  0.00596736],\n",
            "         [-0.03473061, -0.10968219, -0.04877959, ..., -0.05488471,\n",
            "          -0.11308662,  0.03793831]],\n",
            "\n",
            "        [[ 0.04773566,  0.04656745,  0.0481525 , ...,  0.00051234,\n",
            "           0.03907847,  0.0007382 ],\n",
            "         [-0.10505903,  0.04969319, -0.0400987 , ...,  0.00282982,\n",
            "          -0.00934516,  0.01917594],\n",
            "         [-0.03575315,  0.08805496,  0.04297809, ...,  0.0091695 ,\n",
            "          -0.03588593, -0.08609914],\n",
            "         ...,\n",
            "         [ 0.01906223,  0.07436303,  0.07626833, ...,  0.03374417,\n",
            "           0.07133613,  0.08493955],\n",
            "         [ 0.00509425, -0.08202178, -0.05662414, ...,  0.01302694,\n",
            "          -0.05264935, -0.05716946],\n",
            "         [ 0.0163097 , -0.02224415, -0.01146776, ..., -0.07140824,\n",
            "          -0.00566509, -0.04477483]],\n",
            "\n",
            "        [[ 0.07796987,  0.07588302,  0.01237924, ...,  0.02009849,\n",
            "          -0.02059118,  0.05469102],\n",
            "         [ 0.04071045,  0.0071516 , -0.05268161, ...,  0.03664441,\n",
            "           0.0631038 ,  0.02314885],\n",
            "         [ 0.04968103,  0.00043803, -0.05142725, ...,  0.01364337,\n",
            "           0.01907266, -0.05658064],\n",
            "         ...,\n",
            "         [-0.00858913, -0.00994192,  0.07424347, ...,  0.0520219 ,\n",
            "           0.02560978,  0.03593289],\n",
            "         [ 0.03324204, -0.04676272, -0.07069542, ...,  0.03505046,\n",
            "          -0.00245312,  0.02400813],\n",
            "         [-0.04146692, -0.00205907, -0.10450794, ..., -0.03831783,\n",
            "           0.10450839,  0.00391551]]],\n",
            "\n",
            "\n",
            "       [[[-0.00514877,  0.02912277, -0.03948708, ..., -0.02167279,\n",
            "          -0.02827582,  0.03561499],\n",
            "         [-0.02353198, -0.04750117,  0.0207875 , ...,  0.10632936,\n",
            "           0.02261777,  0.10140236],\n",
            "         [ 0.04006269,  0.06308918,  0.13195549, ...,  0.01545365,\n",
            "           0.00761276,  0.0534793 ],\n",
            "         ...,\n",
            "         [ 0.01926526, -0.02930909, -0.02567077, ...,  0.03365592,\n",
            "          -0.04551066,  0.0161863 ],\n",
            "         [ 0.02061573, -0.10134566, -0.01555674, ..., -0.06072018,\n",
            "          -0.04282772, -0.01550638],\n",
            "         [-0.12889607,  0.03562645, -0.04183007, ..., -0.13323967,\n",
            "           0.03834252, -0.01291087]],\n",
            "\n",
            "        [[-0.03119718,  0.00326355,  0.02988866, ..., -0.07212512,\n",
            "          -0.04347561, -0.00463529],\n",
            "         [-0.03964076,  0.02819373, -0.05559928, ..., -0.03424078,\n",
            "           0.03431983, -0.00184709],\n",
            "         [ 0.02890318,  0.04193876,  0.05006855, ..., -0.04125329,\n",
            "           0.03214968, -0.05813947],\n",
            "         ...,\n",
            "         [-0.01814184,  0.03584919, -0.0594216 , ...,  0.04093719,\n",
            "           0.01941615, -0.08069132],\n",
            "         [ 0.00318852, -0.05522528, -0.10214682, ..., -0.02858907,\n",
            "           0.00553198, -0.01677244],\n",
            "         [ 0.05347885, -0.02876524,  0.01381922, ..., -0.02219867,\n",
            "           0.07306755,  0.02571001]],\n",
            "\n",
            "        [[ 0.10501323, -0.04659256, -0.06449654, ...,  0.07879981,\n",
            "           0.01977736, -0.0603638 ],\n",
            "         [ 0.09219714, -0.01738936,  0.09590029, ..., -0.05009121,\n",
            "          -0.02990097, -0.08410823],\n",
            "         [ 0.04680625,  0.0281096 , -0.01728008, ...,  0.00275185,\n",
            "           0.00491769, -0.06426113],\n",
            "         ...,\n",
            "         [-0.00435794,  0.05635481, -0.09661322, ...,  0.00751118,\n",
            "          -0.0137365 , -0.02107178],\n",
            "         [-0.0306154 , -0.05507014, -0.09105086, ..., -0.00843721,\n",
            "          -0.06238855,  0.06094277],\n",
            "         [-0.07796382,  0.09152301,  0.06523895, ..., -0.0218808 ,\n",
            "           0.04406457,  0.07339893]]],\n",
            "\n",
            "\n",
            "       [[[-0.03731072, -0.06301945,  0.12417064, ...,  0.0641863 ,\n",
            "           0.03094645, -0.04891152],\n",
            "         [ 0.02755631,  0.04065992, -0.09762073, ..., -0.02430567,\n",
            "           0.0191052 , -0.0021534 ],\n",
            "         [-0.03643223,  0.02730112,  0.02423226, ..., -0.03921071,\n",
            "           0.02996447,  0.07837781],\n",
            "         ...,\n",
            "         [ 0.06617793, -0.04350889,  0.02045983, ..., -0.04636495,\n",
            "          -0.02122876, -0.0057481 ],\n",
            "         [-0.0719878 , -0.06260481,  0.01786113, ...,  0.02204367,\n",
            "           0.03236138,  0.02816861],\n",
            "         [ 0.04282572, -0.01103205,  0.03204243, ...,  0.01144308,\n",
            "          -0.02646117,  0.06463183]],\n",
            "\n",
            "        [[ 0.08686569, -0.0008115 ,  0.04199741, ...,  0.04891767,\n",
            "           0.02906144,  0.06553175],\n",
            "         [-0.06127107, -0.01208189, -0.06239675, ...,  0.02972644,\n",
            "          -0.04736106, -0.01726418],\n",
            "         [ 0.01611779, -0.00038597, -0.00157436, ..., -0.01124898,\n",
            "          -0.01723474,  0.00173154],\n",
            "         ...,\n",
            "         [-0.00788394, -0.08554488, -0.06100381, ..., -0.02620422,\n",
            "          -0.00773725,  0.07535265],\n",
            "         [-0.02280175, -0.01333874,  0.02057933, ...,  0.00958932,\n",
            "           0.02531437, -0.04987147],\n",
            "         [ 0.03427383,  0.04283918, -0.02403919, ...,  0.04385102,\n",
            "          -0.00699783, -0.03133232]],\n",
            "\n",
            "        [[ 0.00117289, -0.04097577,  0.05866761, ..., -0.0020126 ,\n",
            "           0.00066611, -0.01054747],\n",
            "         [-0.02513642,  0.00393784, -0.02887081, ...,  0.06542686,\n",
            "           0.06150181,  0.0970779 ],\n",
            "         [ 0.01730089, -0.05708067, -0.01512446, ...,  0.03666375,\n",
            "           0.0139602 , -0.01003292],\n",
            "         ...,\n",
            "         [ 0.01257777,  0.0157126 , -0.05263295, ...,  0.04685948,\n",
            "           0.01092237, -0.01298703],\n",
            "         [ 0.01485449,  0.02113101, -0.02219135, ...,  0.04570453,\n",
            "           0.02798656,  0.02972832],\n",
            "         [-0.03206468, -0.1004314 , -0.10011401, ..., -0.07462322,\n",
            "          -0.1142918 ,  0.03064402]]]], dtype=float32)>, <tf.Variable 'gunn2d_24/Variable:0' shape=(1, 1, 1, 240) dtype=float32, numpy=\n",
            "array([[[[-0.01607827, -0.04933568, -0.03288504,  0.04499476,\n",
            "           0.00286027, -0.06397168,  0.0633163 ,  0.00273631,\n",
            "          -0.0262531 , -0.0295029 ,  0.0512844 , -0.07589097,\n",
            "          -0.02010152,  0.0031507 , -0.0533624 ,  0.02141664,\n",
            "           0.00598568,  0.04836158,  0.01505532, -0.0184002 ,\n",
            "           0.00959737, -0.01963694, -0.06732996, -0.04921786,\n",
            "           0.04913996,  0.08098107, -0.01595602,  0.03204513,\n",
            "           0.02739157,  0.03970484,  0.0112373 ,  0.08924492,\n",
            "          -0.01197304, -0.00022327, -0.06335819, -0.04980676,\n",
            "          -0.09502617,  0.01535102, -0.03257371, -0.04644585,\n",
            "           0.00541505, -0.02475724,  0.05272893,  0.01910661,\n",
            "          -0.01086831, -0.02371786, -0.04547198, -0.04039742,\n",
            "          -0.08281622,  0.01735846, -0.03695635,  0.04375842,\n",
            "           0.0673755 , -0.03164992, -0.01118372, -0.04523759,\n",
            "           0.04280831, -0.03741887,  0.06104252,  0.0043584 ,\n",
            "           0.03147836, -0.02040588, -0.02275809,  0.03644066,\n",
            "           0.0862909 ,  0.03282307,  0.05443399,  0.06808417,\n",
            "           0.07601064, -0.02633595, -0.03050636,  0.0345638 ,\n",
            "          -0.0239629 , -0.03131553,  0.09124136, -0.05241717,\n",
            "           0.07088758, -0.00758707,  0.10211863, -0.02122452,\n",
            "           0.08732889, -0.09299459, -0.07592069, -0.01218425,\n",
            "          -0.06118395,  0.01297875,  0.02977442,  0.01241491,\n",
            "           0.06391171,  0.02280363, -0.00907526, -0.07987811,\n",
            "           0.03786022,  0.01166813,  0.03790648,  0.01495074,\n",
            "           0.05828377,  0.03365932, -0.04890251,  0.03519486,\n",
            "          -0.05643217, -0.04666189,  0.07188188, -0.03851016,\n",
            "          -0.05160193,  0.00978871,  0.02112867, -0.01639405,\n",
            "          -0.02482646, -0.02200723,  0.06001612,  0.0211885 ,\n",
            "          -0.0103008 , -0.07905328, -0.02333331,  0.04180174,\n",
            "          -0.01613091, -0.03542328,  0.02328441,  0.08910197,\n",
            "          -0.0022954 , -0.0079543 ,  0.03793719, -0.00422861,\n",
            "           0.01780187,  0.01627473,  0.10389704, -0.08532394,\n",
            "           0.06850993, -0.00138707, -0.00411706, -0.0500419 ,\n",
            "          -0.05547282, -0.0978641 , -0.01044614, -0.0443512 ,\n",
            "           0.029365  ,  0.00218754, -0.0054234 ,  0.04373104,\n",
            "          -0.03948932, -0.0393525 ,  0.01188515, -0.03861609,\n",
            "           0.06950086, -0.13621958,  0.04382356,  0.02947241,\n",
            "          -0.01786388, -0.06950375,  0.00849874, -0.02351561,\n",
            "           0.03734038, -0.00857041, -0.08991159, -0.02049536,\n",
            "           0.00444966, -0.07733514, -0.03064536,  0.01066728,\n",
            "           0.01687662,  0.04740383, -0.00891261, -0.00743612,\n",
            "           0.08325619,  0.01251041, -0.03082584, -0.01520581,\n",
            "          -0.06183387, -0.04253301, -0.02802164, -0.01823503,\n",
            "          -0.03111649,  0.12900993,  0.09441755,  0.03195057,\n",
            "           0.03118725,  0.01709956, -0.04534947, -0.02775925,\n",
            "          -0.07474776, -0.02301048,  0.05798465,  0.08330306,\n",
            "          -0.04256118, -0.06385674,  0.03506573, -0.03529842,\n",
            "          -0.13803242, -0.06887439,  0.05378633, -0.00722088,\n",
            "           0.01843589, -0.05154432, -0.05834734,  0.02344006,\n",
            "           0.07131302, -0.04949939, -0.03076878,  0.01604612,\n",
            "           0.00190805, -0.01564286,  0.01914604, -0.01904726,\n",
            "          -0.02893872,  0.05915235,  0.00296507,  0.04467329,\n",
            "          -0.05739575,  0.02691232,  0.03555676,  0.00626635,\n",
            "          -0.01209571,  0.02067536, -0.10217444, -0.02110475,\n",
            "           0.03386331, -0.06086729,  0.0244554 , -0.06080959,\n",
            "          -0.04699009, -0.06728244,  0.02796076,  0.03494847,\n",
            "          -0.01785631, -0.0058978 ,  0.04986928, -0.01082308,\n",
            "          -0.01915044,  0.09631807,  0.08548912,  0.08035329,\n",
            "           0.01905724,  0.01701589, -0.04714014, -0.00547429,\n",
            "           0.01075157,  0.04791788,  0.05276562, -0.01003211]]]],\n",
            "      dtype=float32)>, DictWrapper({'pad': 1, 'stride': 1})))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkN4sSzMh0Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backup\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Gunn2D_bk(layers.Layer):\n",
        "\n",
        "  def __init__(self, input_channels, expansion_rate=32):\n",
        "    super(Gunn2D, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    self.expansion_rate = expansion_rate\n",
        "    self.hparameters = {\"pad\" : 0, \"stride\": 1}\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.input_channels), initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(shape=(self.input_channels,), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BysAGBh6kxWR",
        "colab_type": "code",
        "outputId": "7f9e87f1-88fc-4e05-8b99-49174387ffa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from tqdm import tqdm\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "# Setting seeds for reproducibility \n",
        "np.random.seed(0)\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "# Dataset: given 2 numbers, predict the sum\n",
        "# Sum 2 numbers from 0 to 10 dataset\n",
        "samples = np.random.randint(0, 9, size=(100,2))\n",
        "targets = np.sum(samples, axis=-1)\n",
        "\n",
        "# Samples for testing\n",
        "samples_test = np.random.randint(0, 9, size=(10,2))\n",
        "targets_test = np.sum(samples_test, axis=-1)\n",
        "\n",
        "# Model\n",
        "x = Input(shape=[2])\n",
        "y = Dense(units=1)(x)\n",
        "model = Model(x, y)\n",
        "\n",
        "# Loss\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # You can get all the crazy and twisted you \n",
        "    # want here no Keras restrictions this time :)\n",
        "    loss_value = K.sum(K.pow((y_true - y_pred), 2))\n",
        "    return loss_value\n",
        "\n",
        "# Optimizer to run the gradients\n",
        "optimizer = Adam(lr=1e-4)\n",
        "\n",
        "# Graph creation\n",
        "# Creating training flow\n",
        "# Ground truth input, samples or X_t\n",
        "y_true = Input(shape=[0])\n",
        "\n",
        "# Prediction\n",
        "y_pred = model(x)\n",
        "\n",
        "# Loss \n",
        "loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "# Operation for getting \n",
        "# gradients and updating weights\n",
        "updates_op = optimizer.get_updates(\n",
        "    params=model.trainable_weights, \n",
        "    loss=loss)\n",
        "\n",
        "# The graph is created, now we need to call it\n",
        "# this would be similar to tf session.run()\n",
        "train = K.function(\n",
        "    inputs=[x, y_true], \n",
        "    outputs=[loss], \n",
        "    updates=updates_op)\n",
        "\n",
        "test = K.function(\n",
        "    inputs=[x, y_true], \n",
        "    outputs=[loss])\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch %s:' % epoch)\n",
        "\n",
        "    # Fancy progress bar\n",
        "    pbar = tqdm(range(len(samples)))\n",
        "\n",
        "    # Storing losses for computing mean\n",
        "    losses_train = []\n",
        "\n",
        "    # Batch loop: batch size=1\n",
        "    for idx in pbar:\n",
        "        sample = samples[idx]\n",
        "        target = targets[idx]\n",
        "\n",
        "        # Adding batch dim since batch=1\n",
        "        sample = np.expand_dims(sample, axis=0)\n",
        "        target = np.expand_dims(target, axis=0)\n",
        "\n",
        "        # To tensors, input of \n",
        "        # K.function must be tensors\n",
        "        sample = K.constant(sample)\n",
        "        target = K.constant(target)\n",
        "\n",
        "        # Running the train graph\n",
        "        loss_train = train([sample, target])\n",
        "        \n",
        "        # Compute loss mean\n",
        "        losses_train.append(loss_train[0])\n",
        "        loss_train_mean = np.mean(losses_train)\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_description('Train Loss: %.3f' % loss_train_mean)\n",
        "\n",
        "    # Testing\n",
        "    losses_test = []\n",
        "    for idx in range(len(samples_test)):\n",
        "        sample_test = samples_test[idx]\n",
        "        target_test = targets_test[idx]\n",
        "\n",
        "        # Adding batch dim since batch=1\n",
        "        sample_test = np.expand_dims(sample_test, axis=0)\n",
        "        target_test = np.expand_dims(target_test, axis=0)\n",
        "\n",
        "        # To tensors\n",
        "        sample_test = K.constant(sample_test)\n",
        "        target_test = K.constant(target_test)\n",
        "        \n",
        "        # Evaluation test graph\n",
        "        loss_test = test([sample_test, target_test])\n",
        "        \n",
        "        # Compute test loss mean\n",
        "        losses_test.append(loss_test[0])\n",
        "    \n",
        "    loss_test_mean = np.mean(losses_test)\n",
        "    print('Test Loss: %.3f' % loss_test_mean)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 97.812: 100%|██████████| 100/100 [00:04<00:00, 23.78it/s]\n",
            "Train Loss: 121.951:   3%|▎         | 3/100 [00:00<00:04, 23.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 66.865\n",
            "Epoch 1:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 96.570: 100%|██████████| 100/100 [00:04<00:00, 21.04it/s]\n",
            "Train Loss: 120.675:   3%|▎         | 3/100 [00:00<00:04, 20.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 65.986\n",
            "Epoch 2:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 95.362: 100%|██████████| 100/100 [00:05<00:00, 18.75it/s]\n",
            "Train Loss: 122.852:   2%|▏         | 2/100 [00:00<00:05, 17.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 65.124\n",
            "Epoch 3:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 94.173: 100%|██████████| 100/100 [00:06<00:00, 16.59it/s]\n",
            "Train Loss: 121.590:   2%|▏         | 2/100 [00:00<00:05, 17.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 64.275\n",
            "Epoch 4:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 92.999: 100%|██████████| 100/100 [00:06<00:00, 14.36it/s]\n",
            "Train Loss: 76.769:   2%|▏         | 2/100 [00:00<00:07, 13.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 63.438\n",
            "Epoch 5:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 91.842: 100%|██████████| 100/100 [00:07<00:00, 12.52it/s]\n",
            "Train Loss: 76.024:   2%|▏         | 2/100 [00:00<00:07, 13.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 62.614\n",
            "Epoch 6:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 90.698: 100%|██████████| 100/100 [00:09<00:00, 10.96it/s]\n",
            "Train Loss: 75.286:   2%|▏         | 2/100 [00:00<00:08, 11.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 61.802\n",
            "Epoch 7:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 89.569: 100%|██████████| 100/100 [00:09<00:00, 10.45it/s]\n",
            "Train Loss: 74.555:   2%|▏         | 2/100 [00:00<00:09, 10.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 61.001\n",
            "Epoch 8:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 88.454: 100%|██████████| 100/100 [00:10<00:00,  9.71it/s]\n",
            "Train Loss: 73.831:   2%|▏         | 2/100 [00:00<00:09, 10.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 60.212\n",
            "Epoch 9:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 87.352: 100%|██████████| 100/100 [00:10<00:00,  9.58it/s]\n",
            "Train Loss: 73.113:   2%|▏         | 2/100 [00:00<00:09, 10.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 59.433\n",
            "Epoch 10:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 86.263: 100%|██████████| 100/100 [00:10<00:00,  9.19it/s]\n",
            "Train Loss: 117.839:   1%|          | 1/100 [00:00<00:10,  9.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 58.665\n",
            "Epoch 11:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 85.188: 100%|██████████| 100/100 [00:11<00:00,  8.56it/s]\n",
            "Train Loss: 116.925:   1%|          | 1/100 [00:00<00:11,  8.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 57.908\n",
            "Epoch 12:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 84.126: 100%|██████████| 100/100 [00:12<00:00,  8.28it/s]\n",
            "Train Loss: 116.018:   1%|          | 1/100 [00:00<00:11,  8.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 57.162\n",
            "Epoch 13:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 83.076: 100%|██████████| 100/100 [00:13<00:00,  7.62it/s]\n",
            "Train Loss: 115.117:   1%|          | 1/100 [00:00<00:13,  7.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 56.426\n",
            "Epoch 14:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 82.039: 100%|██████████| 100/100 [00:14<00:00,  7.02it/s]\n",
            "Train Loss: 114.222:   1%|          | 1/100 [00:00<00:14,  6.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 55.700\n",
            "Epoch 15:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 81.014: 100%|██████████| 100/100 [00:16<00:00,  6.20it/s]\n",
            "Train Loss: 113.334:   1%|          | 1/100 [00:00<00:14,  6.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 54.985\n",
            "Epoch 16:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 80.001: 100%|██████████| 100/100 [00:15<00:00,  6.35it/s]\n",
            "Train Loss: 112.451:   1%|          | 1/100 [00:00<00:15,  6.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 54.279\n",
            "Epoch 17:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 79.000: 100%|██████████| 100/100 [00:16<00:00,  5.92it/s]\n",
            "Train Loss: 111.575:   1%|          | 1/100 [00:00<00:17,  5.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 53.583\n",
            "Epoch 18:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 78.011: 100%|██████████| 100/100 [00:19<00:00,  5.11it/s]\n",
            "Train Loss: 110.704:   1%|          | 1/100 [00:00<00:19,  5.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 52.897\n",
            "Epoch 19:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 77.034: 100%|██████████| 100/100 [00:20<00:00,  4.77it/s]\n",
            "Train Loss: 109.839:   1%|          | 1/100 [00:00<00:19,  5.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 52.220\n",
            "Epoch 20:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 76.068: 100%|██████████| 100/100 [00:21<00:00,  4.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5G_95jVd_dX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1H2Bm4IVcun",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ8F5Lc0lp7O",
        "colab_type": "code",
        "outputId": "ccd69458-8110-49b7-ea1a-b28044af7f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    result = ... # do forward computation\n",
        "    def custom_grad(dy):\n",
        "        grad = ... # compute gradient\n",
        "        return grad\n",
        "    return result, custom_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-187da058f8d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# do forward computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSawda3Q6GZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "First of all, the \"unification\" of the APIs (as you call it) under keras doesn't prevent you from doing things like you did in TensorFlow 1.x. Sessions might be gone but you can still define your model like any python function and train it eagerly without keras (i.e. through tf.GradientTape)\n",
        "\n",
        "Now, if you want to build a keras model with a custom layer that performs a custom operation and has a custom gradient, you should do the following:\n",
        "\n",
        "a) Write a function that performs your custom operation and define your custom gradient. More info on how to do this here.\n",
        "\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    result = ... # do forward computation\n",
        "    def custom_grad(dy):\n",
        "        grad = ... # compute gradient\n",
        "        return grad\n",
        "    return result, custom_grad\n",
        "Note that in the function you should treat x and dy as Tensors and not numpy arrays (i.e. perform tensor operations)\n",
        "\n",
        "b) Create a custom keras layer that performs your custom_op. For this example I'll assume that your layer doesn't have any trainable parameters or change the shape of its input, but it doesn't make much difference if it does. For that you can refer to the guide that you posted check this one.\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(CustomLayer, self).__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        return custom_op(x)  # you don't need to explicitly define the custom gradient\n",
        "                             # as long as you registered it with the previous method\n",
        "Now you can use this layer in a keras model and it will work. For example:\n",
        "\n",
        "inp = tf.keras.layers.Input(input_shape)\n",
        "conv = tf.keras.layers.Conv2D(...)(inp)  # add params like the number of filters\n",
        "cust = CustomLayer()(conv)  # no parameters in custom layer\n",
        "flat = tf.keras.layers.Flatten()(cust)\n",
        "fc = tf.keras.layers.Dense(num_classes)(flat)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[fc])\n",
        "model.compile(loss=..., optimizer=...)  # add loss function and optimizer\n",
        "model.fit(...)  # fit the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z49DzUWUuGx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzWB-qzUUuPD",
        "colab_type": "text"
      },
      "source": [
        "# Stack overflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whph12JzUtiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I am trying to do CIFAR-10 Image Classification using GUNN-15 model based on paper : https://arxiv.org/pdf/1711.09280.pdf\n",
        "\n",
        "\n",
        "I've built GUNN-15 Model in Keras for 10 classes as follows:\n",
        "\n",
        "```\n",
        "def GUNN_15_model(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the GUNN-15 Model.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    X_input = Input(input_shape)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), padding='same', name = 'z1')(X_input) # 32x32x3 -> 32x32x64   ; padding = 1\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(240, (1, 1), strides = (1, 1), padding='valid', name = 'z2')(X) # 32x32x64 -> 32x32x240\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn2')(X)\n",
        "    layer = Activation('relu')\n",
        "    X = layer(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 240, 20, layer.get_weights()))(X)\n",
        "    #X = Lambda(Gunn2D(X, 240, 20))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(300, (1, 1), strides = (1, 1), padding='valid', name = 'z3')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((2, 2), name = 'avg_pool1')(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 300, 25))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(360, (1, 1), strides = (1, 1), padding='valid', name = 'z4')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((2, 2), name = 'avg_pool2')(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 360, 30))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(360, (1, 1), strides = (1, 1), padding='valid', name = 'z5')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((8, 8), name = 'avg_pool3')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(360, activation='softmax', name = 'fc1')(X)\n",
        "    X = Dense(360, activation='softmax', name = 'fc2')(X)\n",
        "    X = Dense(10, activation='softmax', name = 'fc3')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name = 'GUNN-15-Model')\n",
        "    print(model)\n",
        "    return model\n",
        "\n",
        "```\n",
        "\n",
        "The Gunn2D() layers have been inserted at the required positions in the model. These Gunn2D() layers are my custom layers that would take the input as X (input tensor) and filter/weights and output tensor during forward pass, while in backward pass it is supposed to calculate the derivatives of weights and input layers and then update weights and input layers using the output tensor. \n",
        "\n",
        "\n",
        "I have function conv_forward() that does the forward pass for a CNN.\n",
        "\n",
        "```\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    ...\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    return Z, cache\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def conv_backward(dZ, cache):\n",
        "    ...\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The entire function for the above definitions is as below:\n",
        "\n",
        "```\n",
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    s = a_slice_prev * W   # element wise product in python\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = np.sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z =  Z + float(b)\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    \n",
        "    # Retrieve dimensions from W's shape \n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" \n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    n_H = int((n_H_prev + 2*pad -f) // stride ) + 1\n",
        "    n_W = int((n_W_prev + 2*pad -f) // stride ) + 1\n",
        "    \n",
        "    print('Input Shape: {} {} {} {} '.format(m, n_H_prev, n_W_prev, n_C_prev, ))\n",
        "    print('Filter Shape: {} {} {} {} '.format(f, f, n_C_prev, n_C))\n",
        "    print('output Shape: {} {} {} {} '.format(n_H, n_W, stride, pad))\n",
        "    # Initialize the output volume Z with zeros. \n",
        "    #Z = np.zeros(( m, n_H, n_W, n_C ))\n",
        "    #Z = tf.zeros(( m, n_H, n_W, n_C ), tf.float32)\n",
        "    Z = tf.Variable(tf.zeros(( m, n_H, n_W, n_C ), tf.float32), validate_shape=False)\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "\n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume    \n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" \n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
        "                    a_slice_prev = a_prev_pad[ vert_start:vert_end, horiz_start:horiz_end, : ]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "\n",
        "                                            \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          numpy array of shape (1, 1, 1, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve dimensions from W's shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\"\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    \n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    \n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "    db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "    \n",
        "    for i in range(m):                       # loop over the training examples\n",
        "        \n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i, :]\n",
        "        da_prev_pad = dA_prev_pad[i, :]\n",
        "        \n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters \n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                    \n",
        "        # Set the ith training example's dA_prev to the unpaded da_prev_pad : use X[pad:-pad, pad:-pad, :]\n",
        "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "I want to write get the Gunn2D layer call conv_forward() during forward pass and conv_backward() during backward pass while also updating the weights of layer and modifying output tensor during forward pass and backward pass.\n",
        "\n",
        "i have trued to use Lambda function of Keras in the model where I call this function :\n",
        "\n",
        "```\n",
        "@tf.custom_gradient\n",
        "def Gunn2D(A_prev, input_channels, expansion_rate, weights):\n",
        "    def grad(dZ):\n",
        "        print('backpropagation')\n",
        "        dA, dW, db  = conv_backward(dZ, cache)\n",
        "        return dA\n",
        "\n",
        "    print('custom_gradient : {},  weights: '.format(A_prev.shape, weights))\n",
        "    Z, cache = conv_forward(A_prev, W, b, hparameters)\n",
        "    Z = tf.cast(Z, 'float32')\n",
        "    return Z, grad\n",
        "```\n",
        "\n",
        "and Create the model using:\n",
        "\n",
        "```\n",
        "# Create model\n",
        "gunn15model = GUNN_15_model(X_train.shape[1:]) # input: (32, 32, 3)\n",
        "```\n",
        "\n",
        "This gives me the error:\n",
        "\n",
        "```\n",
        "Tensor(\"activation_6/Relu:0\", shape=(?, 32, 32, 240), dtype=float32)\n",
        "custom_gradient : (?, 32, 32, 240),  weights: \n",
        "---------------------------------------------------------------------------\n",
        "NameError                                 Traceback (most recent call last)\n",
        "<ipython-input-16-12c4818aacca> in <module>()\n",
        "----> 1 gunn15model = GUNN_15_model(X_train.shape[1:]) # input: (32, 32, 3)\n",
        "      2 \n",
        "\n",
        "6 frames\n",
        "<ipython-input-15-35d163565c64> in Gunn2D(A_prev, input_channels, expansion_rate, weights)\n",
        "      7 \n",
        "      8     print('custom_gradient : {},  weights: '.format(A_prev.shape, weights))\n",
        "----> 9     Z, cache = conv_forward(A_prev, W, b, hparameters)\n",
        "     10     Z = tf.cast(Z, 'float32')\n",
        "     11     return Z, grad\n",
        "\n",
        "NameError: name 'W' is not defined\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The layer.get_weights() seems to be not passing any value"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}