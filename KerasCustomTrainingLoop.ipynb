{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasCustomTrainingLoop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-778IVk58m",
        "colab_type": "code",
        "outputId": "d1f4e57b-a300-4e2a-eb3c-75afd8775db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "#!pip install -U keras\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 78kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1qA79X_eFy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2255164c-3611-4bbf-e7fe-dd53556bdc03"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "#tf.executing_eagerly()\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IASD7wJ9WOG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    expand = hparameters[\"expand\"]\n",
        "    channels = hparameters[\"channels\"]\n",
        "    depth_batch = channels // expand\n",
        "\n",
        "    # Conv2D for 1 step of gradual update\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    print('Filter Shape: {} '.format(W.shape))\n",
        "    for i in range(depth_batch):\n",
        "        # if you dont add b or not use a registered parameter, tensorflow will give error as follows:\n",
        "        # Gradients do not exist for variables ['layer/Variable:0'] when minimizing the loss\n",
        "        Z = tf.nn.conv2d(A_prev, W, [1, 1, 1, 1], \"SAME\") + b\n",
        "        A_prev = tf.concat([A_prev[:, :, :, :i*expand ], Z, A_prev[:, :, :, i*expand + expand : ]], 3)\n",
        "        print('Output Shape: {} '.format(Z.get_shape()))\n",
        "        print('Input prop : {}'.format(A_prev.shape))\n",
        "                                                \n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return A_prev, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvdJbw_UeMKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.custom_gradient\n",
        "def Gunn2D_3(A_prev, input_channels, expansion_rate, W, b):\n",
        "    #def grad(dZ):\n",
        "    #    print('backpropagation')\n",
        "    #    dA, dW, db  = conv_backward(dZ, cache)\n",
        "    #    return dA\n",
        "\n",
        "    print('custom_gradient : {},  weights: '.format(A_prev.shape, weights))\n",
        "    Z, cache = conv_forward(A_prev, W, b, hparameters)\n",
        "    #Z = tf.cast(Z, 'float32')\n",
        "    return Z, grad\n",
        "\n",
        "\n",
        "class Gunn2D(layers.Layer):\n",
        "\n",
        "  def __init__(self, input_channels, expansion_rate=32):\n",
        "    super(Gunn2D, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    self.expansion_rate = expansion_rate\n",
        "    self.hparameters = {\"pad\" : 1, \"stride\": 1, \"expand\": self.expansion_rate, \"channels\": self.input_channels}\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    print(input_shape)\n",
        "    self.w = self.add_weight(shape=(3, 3, self.input_channels, self.expansion_rate), initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(shape=(1, 1, 1, self.expansion_rate), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    #inputs = Conv2D(240, (1, 1), strides = (1, 1), padding='valid', name = 'zGunn')(inputs)\n",
        "    #print(self.w, self.b)\n",
        "    output, cache = conv_forward(inputs, self.w, self.b, self.hparameters)\n",
        "    #self.w, self.b = cache[1], cache[2]\n",
        "    print(self.w, self.b)\n",
        "    #tf.print(self.w)\n",
        "    #tf.print(self.b)\n",
        "    print('output prop : {}'.format(output.shape))\n",
        "    return output \n",
        "\n",
        "\n",
        "def GunnModel(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the Model.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    print(input_shape)\n",
        "    X_input = Input(input_shape)\n",
        "    print('Before Gunn: {}'.format(X_input.get_shape()))\n",
        "    Gunn2D_layer = Gunn2D(6, 2)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "    X = Gunn2D_layer(X_input)  # The layer's weights are created dynamically the first time the layer is called\n",
        "    print('After Gunn: {}'.format(X.get_shape()))\n",
        "    #print(Gunn2D_layer.kernel, Gunn2D_layer.bias)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(3, activation='softmax', name = 'fc1')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name = 'GunnModel')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7drUaH8ehFk",
        "colab_type": "code",
        "outputId": "7df172a8-9733-4543-dedb-1e3ee464f30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#print(tf.__version__)\n",
        "\n",
        "#x = tf.ones((1, 32, 32, 240))\n",
        "#Gunn2D_layer = Gunn2D(240, 32)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "X_train = tf.ones((50, 5, 5, 6))\n",
        "X_test = tf.ones((20, 5, 5, 6))\n",
        "Y_train = tf.ones((50, 3))\n",
        "Y_test = tf.ones((20, 3))\n",
        "\n",
        "gunnModel = GunnModel(X_train.shape[1:])\n",
        "#sess.run(tf.global_variables_initializer()) # initialize \n",
        "#print(sess.run([gunnModel.output], feed_dict={Z: X_train}))\n",
        "gunnModel.compile(optimizer = \"adam\", loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy])\n",
        "gunnModel.fit(x = X_train , y = Y_train, epochs = 5, steps_per_epoch = (X_train.shape[0]//10))\n",
        "#gunnModel.fit(x = X_train , y = Y_train, epochs = 2, batch_size = 10, steps_per_epoch = (X_train.shape[0]//10))\n",
        "#print(sess.run([gunnModel.output], feed_dict={Z: X_test}))\n",
        "preds = gunnModel.evaluate(x=X_test, y=Y_test)\n",
        "print()\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 5, 6)\n",
            "Before Gunn: (None, 5, 5, 6)\n",
            "(None, 5, 5, 6)\n",
            "Forward prop : (None, 5, 5, 6)\n",
            "Filter Shape: (3, 3, 6, 2) \n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "<tf.Variable 'gunn2d_10/Variable:0' shape=(3, 3, 6, 2) dtype=float32> <tf.Variable 'gunn2d_10/Variable:0' shape=(1, 1, 1, 2) dtype=float32>\n",
            "output prop : (None, 5, 5, 6)\n",
            "After Gunn: (None, 5, 5, 6)\n",
            "Epoch 1/5\n",
            "Forward prop : (10, 5, 5, 6)\n",
            "Filter Shape: (3, 3, 6, 2) \n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "<tf.Variable 'gunn2d_10/Variable:0' shape=(3, 3, 6, 2) dtype=float32> <tf.Variable 'gunn2d_10/Variable:0' shape=(1, 1, 1, 2) dtype=float32>\n",
            "output prop : (10, 5, 5, 6)\n",
            "Forward prop : (10, 5, 5, 6)\n",
            "Filter Shape: (3, 3, 6, 2) \n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "Output Shape: (10, 5, 5, 2) \n",
            "Input prop : (10, 5, 5, 6)\n",
            "<tf.Variable 'gunn2d_10/Variable:0' shape=(3, 3, 6, 2) dtype=float32> <tf.Variable 'gunn2d_10/Variable:0' shape=(1, 1, 1, 2) dtype=float32>\n",
            "output prop : (10, 5, 5, 6)\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.3289 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.3464 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.3872 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.4243 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.4426 - categorical_accuracy: 0.0000e+00\n",
            "Forward prop : (None, 5, 5, 6)\n",
            "Filter Shape: (3, 3, 6, 2) \n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "Output Shape: (None, 5, 5, 2) \n",
            "Input prop : (None, 5, 5, 6)\n",
            "<tf.Variable 'gunn2d_10/Variable:0' shape=(3, 3, 6, 2) dtype=float32> <tf.Variable 'gunn2d_10/Variable:0' shape=(1, 1, 1, 2) dtype=float32>\n",
            "output prop : (None, 5, 5, 6)\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.4442 - categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Loss = 3.4441680908203125\n",
            "Test Accuracy = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzJZqA3iDno2",
        "colab_type": "text"
      },
      "source": [
        "# Putting the GUNN layer it together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MonVxC6PDswD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a751b8b8-5e40-4bf5-c0d5-1872db1a540e"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    expand = hparameters[\"expand\"]\n",
        "    channels = hparameters[\"channels\"]\n",
        "    depth_batch = channels // expand\n",
        "\n",
        "    # Conv2D for 1 step of gradual update\n",
        "    for i in range(depth_batch):\n",
        "        # if you dont add b or not use a registered parameter, tensorflow will give error as follows:\n",
        "        # Gradients do not exist for variables ['layer/Variable:0'] when minimizing the loss\n",
        "        Z = tf.nn.conv2d(A_prev, W, [1, 1, 1, 1], \"SAME\") + b\n",
        "        A_prev = tf.concat([A_prev[:, :, :, :i*expand ], Z, A_prev[:, :, :, i*expand + expand : ]], 3)\n",
        "                                                \n",
        "    return A_prev\n",
        "\n",
        "\n",
        "class Gunn2D(layers.Layer):\n",
        "\n",
        "  def __init__(self, input_channels, expansion_rate=32):\n",
        "    super(Gunn2D, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    self.expansion_rate = expansion_rate\n",
        "    self.hparameters = {\"expand\": self.expansion_rate, \"channels\": self.input_channels}\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(3, 3, self.input_channels, self.expansion_rate), initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(shape=(1, 1, 1, self.expansion_rate), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    output = conv_forward(inputs, self.w, self.b, self.hparameters)\n",
        "    return output \n",
        "\n",
        "\n",
        "def GunnModel(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the Model.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    X_input = Input(input_shape)\n",
        "    Gunn2D_layer = Gunn2D(6, 2)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "    X = Gunn2D_layer(X_input)  # The layer's weights are created dynamically the first time the layer is called\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(3, activation='softmax', name = 'fc1')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name = 'GunnModel')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9PtqUS9EDBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e240ba7e-ef72-4274-ae8e-8a56addb518b"
      },
      "source": [
        "X_train = tf.ones((50, 5, 5, 6))\n",
        "X_test = tf.ones((20, 5, 5, 6))\n",
        "Y_train = tf.ones((50, 3))\n",
        "Y_test = tf.ones((20, 3))\n",
        "\n",
        "gunnModel = GunnModel(X_train.shape[1:])\n",
        "gunnModel.compile(optimizer = \"adam\", loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy])\n",
        "gunnModel.fit(x = X_train , y = Y_train, epochs = 5, steps_per_epoch = (X_train.shape[0]//10))\n",
        "preds = gunnModel.evaluate(x=X_test, y=Y_test)\n",
        "print()\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.3093 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.3327 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.3868 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.4880 - categorical_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 3.6380 - categorical_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7450 - categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Loss = 3.7450389862060547\n",
            "Test Accuracy = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5zjj3dTEy80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "de3e62bd-a384-409b-fe55-5bf83455fd03"
      },
      "source": [
        "gunnModel.summary()\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"GunnModel\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 5, 5, 6)]         0         \n",
            "_________________________________________________________________\n",
            "gunn2d (Gunn2D)              (None, 5, 5, 6)           110       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 3)                 453       \n",
            "=================================================================\n",
            "Total params: 563\n",
            "Trainable params: 563\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Far8Sj84FOew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e8bd4ff9-8fb2-4e14-fb99-507b4decb885"
      },
      "source": [
        "tf.keras.utils.plot_model(gunnModel, 'gunnModel_model.png', show_shapes=True)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAGVCAIAAAByzt3XAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nkg6CIAb1L+GzeMXso8ubAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvHQ7V5RFaUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDxgPutJocE",
        "colab_type": "text"
      },
      "source": [
        "# Backup without GUNN implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L3PeSwBhIWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    #paddings = tf.constant([[1, 0,],[2, pad,], [3, pad], [4, 0]])\n",
        "    paddings = tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]])\n",
        "    X_pad = tf.pad(X, paddings, 'CONSTANT')\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    \n",
        "    #print('a_slice_prev: {} '.format(a_slice_prev.shape))\n",
        "    #print('W: {} '.format(W.shape))\n",
        "    s = tf.multiply(a_slice_prev, W)   # element wise product in python\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = tf.math.reduce_sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z =  Z + b\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward_back(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    m = 1\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    \n",
        "    # Retrieve dimensions from W's shape \n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" \n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    expand = hparameters[\"expand\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    n_H = int((n_H_prev + 2*pad -f) // stride ) + 1\n",
        "    n_W = int((n_W_prev + 2*pad -f) // stride ) + 1\n",
        "    \n",
        "    print('Input Shape: {} {} {} {} '.format(m, n_H_prev, n_W_prev, n_C_prev, ))\n",
        "    print('Filter Shape: {} {} {} {} '.format(f, f, n_C_prev, n_C))\n",
        "    print('output Shape: {} {} {} {} '.format(n_H, n_W, stride, pad))\n",
        "    # Initialize the output volume Z with zeros. \n",
        "    #Z = np.zeros(( m, n_H, n_W, n_C ))\n",
        "    #Z = tf.zeros(( m, n_H, n_W, n_C ), tf.float32)\n",
        "    \n",
        "    Z = tf.compat.v1.placeholder(tf.float32, shape=(m, n_H, n_W, n_C))\n",
        "\n",
        "    ####Z = tf.Variable(tf.zeros(( m, n_H, n_W, n_C ), tf.float32), validate_shape=False)\n",
        "    #Z = tf.Variable(initial_value=tf.zeros(( m, n_H, n_W, n_C ), shape=( m, n_H, n_W, n_C ), validate_shape=False, dtype=tf.float32)\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "    print('Input Shape after pad: {} '.format(A_prev_pad.shape))\n",
        "    print('tf.shape(Z): {} '.format(tf.shape(Z)))\n",
        "\n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        #print(A_prev_pad.shape)\n",
        "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
        "        print(a_prev_pad.shape)\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume    \n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    #print(m, h, w, c)\n",
        "                    # Find the corners of the current \"slice\" \n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
        "                    a_slice_prev = a_prev_pad[ vert_start:vert_end, horiz_start:horiz_end, : ]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
        "                    #Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    scalar = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    Z[i, h, w, c].assign(scalar)\n",
        "                    \n",
        "\n",
        "                                            \n",
        "    # Making sure your output shape is correct\n",
        "    print(tf.shape(Z))\n",
        "    print(m, n_H, n_W, n_C)\n",
        "    #assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkN4sSzMh0Kk",
        "colab_type": "code",
        "outputId": "8f3f3fe2-4d79-4f3e-8dec-7a9d42b22773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Gunn2D_bk(layers.Layer):\n",
        "\n",
        "  def __init__(self, input_channels, expansion_rate=32):\n",
        "    super(Gunn2D, self).__init__()\n",
        "    self.input_channels = input_channels\n",
        "    self.expansion_rate = expansion_rate\n",
        "    self.hparameters = {\"pad\" : 0, \"stride\": 1}\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.input_channels), initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(shape=(self.input_channels,), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "#x = tf.ones((1, 32, 32, 240))\n",
        "#Gunn2D_layer = Gunn2D(240, 32)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "x = tf.ones((1, 5, 5, 6))\n",
        "Gunn2D_layer = Gunn2D(6, 2)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "y = Gunn2D_layer(x)  # The layer's weights are created dynamically the first time the layer is called\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "(1, 5, 5, 6)\n",
            "WARNING:tensorflow:Entity <bound method Gunn2D.call of <__main__.Gunn2D object at 0x7fe536dbb748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Gunn2D.call of <__main__.Gunn2D object at 0x7fe536dbb748>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method Gunn2D.call of <__main__.Gunn2D object at 0x7fe536dbb748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Gunn2D.call of <__main__.Gunn2D object at 0x7fe536dbb748>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Forward prop : (1, 5, 5, 6)\n",
            "Filter Shape: (3, 3, 6, 2) \n",
            "Output Shape: (1, 5, 5, 2) \n",
            "Input prop : (1, 5, 5, 6)\n",
            "Output Shape: (1, 5, 5, 2) \n",
            "Input prop : (1, 5, 5, 6)\n",
            "Tensor(\"gunn2d_13/Conv2D_1:0\", shape=(1, 5, 5, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxsspkHIJkzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    #paddings = tf.constant([[1, 0,],[2, pad,], [3, pad], [4, 0]])\n",
        "    paddings = tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]])\n",
        "    X_pad = tf.pad(X, paddings, 'CONSTANT')\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    \n",
        "    #print('a_slice_prev: {} '.format(a_slice_prev.shape))\n",
        "    #print('W: {} '.format(W.shape))\n",
        "    s = tf.multiply(a_slice_prev, W)   # element wise product in python\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = tf.math.reduce_sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z =  Z + float(b)\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    \n",
        "    # Retrieve dimensions from W's shape \n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" \n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    n_H = int((n_H_prev + 2*pad -f) // stride ) + 1\n",
        "    n_W = int((n_W_prev + 2*pad -f) // stride ) + 1\n",
        "    \n",
        "    print('Input Shape: {} {} {} {} '.format(m, n_H_prev, n_W_prev, n_C_prev, ))\n",
        "    print('Filter Shape: {} {} {} {} '.format(f, f, n_C_prev, n_C))\n",
        "    print('output Shape: {} {} {} {} '.format(n_H, n_W, stride, pad))\n",
        "    # Initialize the output volume Z with zeros. \n",
        "    #Z = np.zeros(( m, n_H, n_W, n_C ))\n",
        "    #Z = tf.zeros(( m, n_H, n_W, n_C ), tf.float32)\n",
        "    Z = tf.Variable(tf.zeros(( m, n_H, n_W, n_C ), tf.float32), validate_shape=False)\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "    print('Input Shape after pad: {} '.format(A_prev_pad.shape))\n",
        "\n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        #print(A_prev_pad.shape)\n",
        "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
        "        print(a_prev_pad.shape)\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume    \n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    #print(m, h, w, c)\n",
        "                    # Find the corners of the current \"slice\" \n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
        "                    a_slice_prev = a_prev_pad[ vert_start:vert_end, horiz_start:horiz_end, : ]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
        "                    #Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    scalar = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "                    Z[i, h, w, c].assign(scalar)\n",
        "                    \n",
        "\n",
        "                                            \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BysAGBh6kxWR",
        "colab_type": "code",
        "outputId": "5c27a037-6b74-4093-98fd-f0ad744922db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from tqdm import tqdm\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "# Setting seeds for reproducibility \n",
        "np.random.seed(0)\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "# Dataset: given 2 numbers, predict the sum\n",
        "# Sum 2 numbers from 0 to 10 dataset\n",
        "samples = np.random.randint(0, 9, size=(100,2))\n",
        "targets = np.sum(samples, axis=-1)\n",
        "\n",
        "# Samples for testing\n",
        "samples_test = np.random.randint(0, 9, size=(10,2))\n",
        "targets_test = np.sum(samples_test, axis=-1)\n",
        "\n",
        "# Model\n",
        "x = Input(shape=[2])\n",
        "y = Dense(units=1)(x)\n",
        "model = Model(x, y)\n",
        "\n",
        "# Loss\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # You can get all the crazy and twisted you \n",
        "    # want here no Keras restrictions this time :)\n",
        "    loss_value = K.sum(K.pow((y_true - y_pred), 2))\n",
        "    return loss_value\n",
        "\n",
        "# Optimizer to run the gradients\n",
        "optimizer = Adam(lr=1e-4)\n",
        "\n",
        "# Graph creation\n",
        "# Creating training flow\n",
        "# Ground truth input, samples or X_t\n",
        "y_true = Input(shape=[0])\n",
        "\n",
        "# Prediction\n",
        "y_pred = model(x)\n",
        "\n",
        "# Loss \n",
        "loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "# Operation for getting \n",
        "# gradients and updating weights\n",
        "updates_op = optimizer.get_updates(\n",
        "    params=model.trainable_weights, \n",
        "    loss=loss)\n",
        "\n",
        "# The graph is created, now we need to call it\n",
        "# this would be similar to tf session.run()\n",
        "train = K.function(\n",
        "    inputs=[x, y_true], \n",
        "    outputs=[loss], \n",
        "    updates=updates_op)\n",
        "\n",
        "test = K.function(\n",
        "    inputs=[x, y_true], \n",
        "    outputs=[loss])\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch %s:' % epoch)\n",
        "\n",
        "    # Fancy progress bar\n",
        "    pbar = tqdm(range(len(samples)))\n",
        "\n",
        "    # Storing losses for computing mean\n",
        "    losses_train = []\n",
        "\n",
        "    # Batch loop: batch size=1\n",
        "    for idx in pbar:\n",
        "        sample = samples[idx]\n",
        "        target = targets[idx]\n",
        "\n",
        "        # Adding batch dim since batch=1\n",
        "        sample = np.expand_dims(sample, axis=0)\n",
        "        target = np.expand_dims(target, axis=0)\n",
        "\n",
        "        # To tensors, input of \n",
        "        # K.function must be tensors\n",
        "        sample = K.constant(sample)\n",
        "        target = K.constant(target)\n",
        "\n",
        "        # Running the train graph\n",
        "        loss_train = train([sample, target])\n",
        "        \n",
        "        # Compute loss mean\n",
        "        losses_train.append(loss_train[0])\n",
        "        loss_train_mean = np.mean(losses_train)\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_description('Train Loss: %.3f' % loss_train_mean)\n",
        "\n",
        "    # Testing\n",
        "    losses_test = []\n",
        "    for idx in range(len(samples_test)):\n",
        "        sample_test = samples_test[idx]\n",
        "        target_test = targets_test[idx]\n",
        "\n",
        "        # Adding batch dim since batch=1\n",
        "        sample_test = np.expand_dims(sample_test, axis=0)\n",
        "        target_test = np.expand_dims(target_test, axis=0)\n",
        "\n",
        "        # To tensors\n",
        "        sample_test = K.constant(sample_test)\n",
        "        target_test = K.constant(target_test)\n",
        "        \n",
        "        # Evaluation test graph\n",
        "        loss_test = test([sample_test, target_test])\n",
        "        \n",
        "        # Compute test loss mean\n",
        "        losses_test.append(loss_test[0])\n",
        "    \n",
        "    loss_test_mean = np.mean(losses_test)\n",
        "    print('Test Loss: %.3f' % loss_test_mean)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3cb4615e9746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Setting seeds for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Dataset: given 2 numbers, predict the sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5G_95jVd_dX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1H2Bm4IVcun",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ8F5Lc0lp7O",
        "colab_type": "code",
        "outputId": "ccd69458-8110-49b7-ea1a-b28044af7f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    result = ... # do forward computation\n",
        "    def custom_grad(dy):\n",
        "        grad = ... # compute gradient\n",
        "        return grad\n",
        "    return result, custom_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-187da058f8d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# do forward computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;31m# compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSawda3Q6GZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "First of all, the \"unification\" of the APIs (as you call it) under keras doesn't prevent you from doing things like you did in TensorFlow 1.x. Sessions might be gone but you can still define your model like any python function and train it eagerly without keras (i.e. through tf.GradientTape)\n",
        "\n",
        "Now, if you want to build a keras model with a custom layer that performs a custom operation and has a custom gradient, you should do the following:\n",
        "\n",
        "a) Write a function that performs your custom operation and define your custom gradient. More info on how to do this here.\n",
        "\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    result = ... # do forward computation\n",
        "    def custom_grad(dy):\n",
        "        grad = ... # compute gradient\n",
        "        return grad\n",
        "    return result, custom_grad\n",
        "Note that in the function you should treat x and dy as Tensors and not numpy arrays (i.e. perform tensor operations)\n",
        "\n",
        "b) Create a custom keras layer that performs your custom_op. For this example I'll assume that your layer doesn't have any trainable parameters or change the shape of its input, but it doesn't make much difference if it does. For that you can refer to the guide that you posted check this one.\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(CustomLayer, self).__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        return custom_op(x)  # you don't need to explicitly define the custom gradient\n",
        "                             # as long as you registered it with the previous method\n",
        "Now you can use this layer in a keras model and it will work. For example:\n",
        "\n",
        "inp = tf.keras.layers.Input(input_shape)\n",
        "conv = tf.keras.layers.Conv2D(...)(inp)  # add params like the number of filters\n",
        "cust = CustomLayer()(conv)  # no parameters in custom layer\n",
        "flat = tf.keras.layers.Flatten()(cust)\n",
        "fc = tf.keras.layers.Dense(num_classes)(flat)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[fc])\n",
        "model.compile(loss=..., optimizer=...)  # add loss function and optimizer\n",
        "model.fit(...)  # fit the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z49DzUWUuGx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzWB-qzUUuPD",
        "colab_type": "text"
      },
      "source": [
        "# Stack overflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whph12JzUtiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I am trying to do CIFAR-10 Image Classification using GUNN-15 model based on paper : https://arxiv.org/pdf/1711.09280.pdf\n",
        "\n",
        "\n",
        "I've built GUNN-15 Model in Keras for 10 classes as follows:\n",
        "\n",
        "```\n",
        "def GUNN_15_model(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the GUNN-15 Model.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    X_input = Input(input_shape)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), padding='same', name = 'z1')(X_input) # 32x32x3 -> 32x32x64   ; padding = 1\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(240, (1, 1), strides = (1, 1), padding='valid', name = 'z2')(X) # 32x32x64 -> 32x32x240\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn2')(X)\n",
        "    layer = Activation('relu')\n",
        "    X = layer(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 240, 20, layer.get_weights()))(X)\n",
        "    #X = Lambda(Gunn2D(X, 240, 20))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(300, (1, 1), strides = (1, 1), padding='valid', name = 'z3')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((2, 2), name = 'avg_pool1')(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 300, 25))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(360, (1, 1), strides = (1, 1), padding='valid', name = 'z4')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((2, 2), name = 'avg_pool2')(X)\n",
        "    print(X)\n",
        "    X = Lambda(lambda x: Gunn2D(X, 360, 30))(X)\n",
        "    print(X)\n",
        "    X = Conv2D(360, (1, 1), strides = (1, 1), padding='valid', name = 'z5')(X)\n",
        "    X = BatchNormalization(axis = 3 , name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = AveragePooling2D((8, 8), name = 'avg_pool3')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(360, activation='softmax', name = 'fc1')(X)\n",
        "    X = Dense(360, activation='softmax', name = 'fc2')(X)\n",
        "    X = Dense(10, activation='softmax', name = 'fc3')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name = 'GUNN-15-Model')\n",
        "    print(model)\n",
        "    return model\n",
        "\n",
        "```\n",
        "\n",
        "The Gunn2D() layers have been inserted at the required positions in the model. These Gunn2D() layers are my custom layers that would take the input as X (input tensor) and filter/weights and output tensor during forward pass, while in backward pass it is supposed to calculate the derivatives of weights and input layers and then update weights and input layers using the output tensor. \n",
        "\n",
        "\n",
        "I have function conv_forward() that does the forward pass for a CNN.\n",
        "\n",
        "```\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    ...\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    return Z, cache\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def conv_backward(dZ, cache):\n",
        "    ...\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The entire function for the above definitions is as below:\n",
        "\n",
        "```\n",
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    s = a_slice_prev * W   # element wise product in python\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = np.sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z =  Z + float(b)\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    print('Forward prop : {}'.format(A_prev.shape))\n",
        "    \n",
        "    # Retrieve dimensions from W's shape \n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" \n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    n_H = int((n_H_prev + 2*pad -f) // stride ) + 1\n",
        "    n_W = int((n_W_prev + 2*pad -f) // stride ) + 1\n",
        "    \n",
        "    print('Input Shape: {} {} {} {} '.format(m, n_H_prev, n_W_prev, n_C_prev, ))\n",
        "    print('Filter Shape: {} {} {} {} '.format(f, f, n_C_prev, n_C))\n",
        "    print('output Shape: {} {} {} {} '.format(n_H, n_W, stride, pad))\n",
        "    # Initialize the output volume Z with zeros. \n",
        "    #Z = np.zeros(( m, n_H, n_W, n_C ))\n",
        "    #Z = tf.zeros(( m, n_H, n_W, n_C ), tf.float32)\n",
        "    Z = tf.Variable(tf.zeros(( m, n_H, n_W, n_C ), tf.float32), validate_shape=False)\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "\n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume    \n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" \n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad \n",
        "                    a_slice_prev = a_prev_pad[ vert_start:vert_end, horiz_start:horiz_end, : ]\n",
        "                    \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
        "\n",
        "                                            \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          numpy array of shape (1, 1, 1, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve dimensions from W's shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\"\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    \n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    \n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "    db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "    \n",
        "    for i in range(m):                       # loop over the training examples\n",
        "        \n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i, :]\n",
        "        da_prev_pad = dA_prev_pad[i, :]\n",
        "        \n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters \n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                    \n",
        "        # Set the ith training example's dA_prev to the unpaded da_prev_pad : use X[pad:-pad, pad:-pad, :]\n",
        "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "I want to write get the Gunn2D layer call conv_forward() during forward pass and conv_backward() during backward pass while also updating the weights of layer and modifying output tensor during forward pass and backward pass.\n",
        "\n",
        "i have trued to use Lambda function of Keras in the model where I call this function :\n",
        "\n",
        "```\n",
        "@tf.custom_gradient\n",
        "def Gunn2D(A_prev, input_channels, expansion_rate, weights):\n",
        "    def grad(dZ):\n",
        "        print('backpropagation')\n",
        "        dA, dW, db  = conv_backward(dZ, cache)\n",
        "        return dA\n",
        "\n",
        "    print('custom_gradient : {},  weights: '.format(A_prev.shape, weights))\n",
        "    Z, cache = conv_forward(A_prev, W, b, hparameters)\n",
        "    Z = tf.cast(Z, 'float32')\n",
        "    return Z, grad\n",
        "```\n",
        "\n",
        "and Create the model using:\n",
        "\n",
        "```\n",
        "# Create model\n",
        "gunn15model = GUNN_15_model(X_train.shape[1:]) # input: (32, 32, 3)\n",
        "```\n",
        "\n",
        "This gives me the error:\n",
        "\n",
        "```\n",
        "Tensor(\"activation_6/Relu:0\", shape=(?, 32, 32, 240), dtype=float32)\n",
        "custom_gradient : (?, 32, 32, 240),  weights: \n",
        "---------------------------------------------------------------------------\n",
        "NameError                                 Traceback (most recent call last)\n",
        "<ipython-input-16-12c4818aacca> in <module>()\n",
        "----> 1 gunn15model = GUNN_15_model(X_train.shape[1:]) # input: (32, 32, 3)\n",
        "      2 \n",
        "\n",
        "6 frames\n",
        "<ipython-input-15-35d163565c64> in Gunn2D(A_prev, input_channels, expansion_rate, weights)\n",
        "      7 \n",
        "      8     print('custom_gradient : {},  weights: '.format(A_prev.shape, weights))\n",
        "----> 9     Z, cache = conv_forward(A_prev, W, b, hparameters)\n",
        "     10     Z = tf.cast(Z, 'float32')\n",
        "     11     return Z, grad\n",
        "\n",
        "NameError: name 'W' is not defined\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The layer.get_weights() seems to be not passing any value"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}